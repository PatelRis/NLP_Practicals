{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Word Tokenization","metadata":{"editable":false}},{"cell_type":"markdown","source":"Word tokenization (also called word segmentation) is the problem of dividing a string of written language into its component words.","metadata":{"editable":false}},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nsentence = \"Books are on the table\"\nwords = word_tokenize(sentence)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T16:07:56.511572Z","iopub.execute_input":"2021-11-16T16:07:56.511966Z","iopub.status.idle":"2021-11-16T16:07:58.325083Z","shell.execute_reply.started":"2021-11-16T16:07:56.511843Z","shell.execute_reply":"2021-11-16T16:07:58.324233Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"print(words)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2021-11-16T16:08:00.097048Z","iopub.execute_input":"2021-11-16T16:08:00.097367Z","iopub.status.idle":"2021-11-16T16:08:00.103328Z","shell.execute_reply.started":"2021-11-16T16:08:00.097334Z","shell.execute_reply":"2021-11-16T16:08:00.102172Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Lower casing","metadata":{"editable":false}},{"cell_type":"markdown","source":"Converting a word to lower case (NLP -> nlp).\nWords like Book and book mean the same but when not converted to the lower case those two are represented as two\ndifferent words in the vector space model (resulting in more dimensions).","metadata":{"editable":false}},{"cell_type":"code","source":"sentence = \"Backgammon is one of the oldest known board games. Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East.\"\nsentence = sentence.lower()\nprint(sentence)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2021-11-16T16:08:07.219740Z","iopub.execute_input":"2021-11-16T16:08:07.220146Z","iopub.status.idle":"2021-11-16T16:08:07.226198Z","shell.execute_reply.started":"2021-11-16T16:08:07.220103Z","shell.execute_reply":"2021-11-16T16:08:07.224862Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Stop word removal","metadata":{"editable":false}},{"cell_type":"markdown","source":"Stop words are very commonly used words (a, an, the, etc.) in the documents. These words do not\nreally signify any importance as they do not help in distinguishing two documents.\n","metadata":{"execution":{"iopub.status.busy":"2021-11-16T11:58:40.152752Z","iopub.execute_input":"2021-11-16T11:58:40.153795Z","iopub.status.idle":"2021-11-16T11:58:40.159532Z","shell.execute_reply.started":"2021-11-16T11:58:40.153742Z","shell.execute_reply":"2021-11-16T11:58:40.158497Z"},"editable":false}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nsentence = \"Machine Learning is cool!\"\nstop_words = set(stopwords.words('english'))\nword_tokens = word_tokenize(sentence)\nfiltered_sentence = [w for w in word_tokens if not w in stop_words]\nprint(filtered_sentence)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2021-11-16T16:08:07.889731Z","iopub.execute_input":"2021-11-16T16:08:07.890076Z","iopub.status.idle":"2021-11-16T16:08:07.903078Z","shell.execute_reply.started":"2021-11-16T16:08:07.890038Z","shell.execute_reply":"2021-11-16T16:08:07.902055Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Stemming","metadata":{"editable":false}},{"cell_type":"markdown","source":" It is a process of transforming a word to its root form.","metadata":{"editable":false}},{"cell_type":"code","source":"import nltk\nfrom nltk.stem import PorterStemmer\nps = PorterStemmer()\nsentence = \"Machine Learning is cool\"\nfor word in sentence.split():\n    print(ps.stem(word))","metadata":{"editable":false,"execution":{"iopub.status.busy":"2021-11-16T16:08:08.779757Z","iopub.execute_input":"2021-11-16T16:08:08.780409Z","iopub.status.idle":"2021-11-16T16:08:08.787531Z","shell.execute_reply.started":"2021-11-16T16:08:08.780360Z","shell.execute_reply":"2021-11-16T16:08:08.786599Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Lemmatization","metadata":{"editable":false}},{"cell_type":"markdown","source":"Unlike stemming, lemmatization reduces the words to a word existing in the language.\n","metadata":{"editable":false}},{"cell_type":"code","source":"import nltk\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\nprint(lemmatizer.lemmatize(\"Machine\", pos='n'))\nprint(lemmatizer.lemmatize(\"caring\", pos='v'))","metadata":{"editable":false,"execution":{"iopub.status.busy":"2021-11-16T16:08:10.039653Z","iopub.execute_input":"2021-11-16T16:08:10.040430Z","iopub.status.idle":"2021-11-16T16:08:12.248108Z","shell.execute_reply.started":"2021-11-16T16:08:10.040389Z","shell.execute_reply":"2021-11-16T16:08:12.247040Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]}]}